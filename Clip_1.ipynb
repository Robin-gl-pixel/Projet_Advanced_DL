{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Nsoh7QP8qRbq",
      "metadata": {
        "id": "Nsoh7QP8qRbq"
      },
      "source": [
        "# Copy of data exporting an proccessing Code\n",
        "****\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "88aav2D8qZpj",
      "metadata": {
        "id": "88aav2D8qZpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062d7d1e-9363-4fc4-f996-b6bfba77052d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n",
            "There are 20 different categories, which are:\n",
            "['BearHead', 'CatHead', 'ChickenHead', 'CowHead', 'DeerHead', 'DogHead', 'DuckHead', 'EagleHead', 'ElephantHead', 'HumanHead', 'LionHead', 'MonkeyHead', 'MouseHead', 'PandaHead', 'PigHead', 'PigeonHead', 'RabbitHead', 'SheepHead', 'TigerHead', 'WolfHead']\n",
            "\n",
            "\n",
            "The chosen categories for One-Shot Learning are: ['DuckHead', 'WolfHead', 'PigHead', 'LionHead', 'EagleHead']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 502 files [00:00, 2130.62 files/s]\n",
            "Copying files: 1905 files [00:01, 1884.74 files/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install split-folders\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import splitfolders #(pip install split-folders)\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "## Downloading the database\n",
        "\n",
        "# If you do not posess the AnimalFace.zip archive, this cells downloads it\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data/')\n",
        "    \n",
        "if not os.path.exists('data/images/') or len(os.listdir('data/images/')) == 0:\n",
        "    if not os.path.exists('data/AnimalFace.zip'):\n",
        "        url = 'https://vcla.stat.ucla.edu/people/zhangzhang-si/HiT/AnimalFace.zip'\n",
        "        urllib.request.urlretrieve(url, 'data/AnimalFace.zip')\n",
        "    with zipfile.ZipFile('data/AnimalFace.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data/')\n",
        "    os.rename('data/Image/', 'data/images/')\n",
        "\n",
        "if os.path.exists('./data/images/Natural'):\n",
        "    shutil.rmtree('./data/images/Natural')\n",
        "\n",
        "# Download the zip and unzip in a folder named Images\n",
        "# And we delete the Natural folder which contains only 8 images non related to AnimalFaces\n",
        "\n",
        "## Chosing the categories for One Shot Learning\n",
        "\n",
        "categories = os.listdir('data/images')\n",
        "if os.path.exists('data/OneShotImages/'):\n",
        "    oneshot_cats = sorted(os.listdir('data/OneShotImages/'))\n",
        "    categories += oneshot_cats\n",
        "categories = sorted([cat for cat in categories])\n",
        "print(f'There are {len(categories)} different categories, which are:', categories, '\\n', sep='\\n')\n",
        "\n",
        "if not os.path.exists('data/OneShotImages/'):\n",
        "    np.random.seed(42)\n",
        "    oneshot_cats = np.random.choice(len(categories), 5)\n",
        "    oneshot_cats = [categories[num] for num in oneshot_cats]\n",
        "print('The chosen categories for One-Shot Learning are:', oneshot_cats)\n",
        "\n",
        "train_cats = list(set(categories) - set(oneshot_cats))\n",
        "\n",
        "if not os.path.exists('data/OneShotImages/'):\n",
        "    os.makedirs('data/OneShotImages/')\n",
        "    for cat in oneshot_cats:\n",
        "        shutil.move('data/images/' + cat, 'data/OneShotImages/' + cat)\n",
        "\n",
        "## Separating the folders into train and test\n",
        "\n",
        "IMAGES_PATH = 'data/images/'\n",
        "IMAGES_OS_PATH = 'data/OneShotImages/'\n",
        "\n",
        "splitfolders.fixed(IMAGES_OS_PATH, output='data/', seed=42, fixed=(1, 0, -1), group_prefix=None, move=False)\n",
        "shutil.rmtree('data/val/')\n",
        "os.rename('data/train/', 'data/train_os/')\n",
        "os.rename('data/test/', 'data/test_os/')\n",
        "\n",
        "splitfolders.ratio(IMAGES_PATH, output='data/', seed=42, ratio=(.8, .0, .2), group_prefix=None, move=False)\n",
        "shutil.rmtree('data/val/')\n",
        "\n",
        "## Visualization of the database\n",
        "\n",
        "batch_size = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5LV9zvJgqdHb",
      "metadata": {
        "id": "5LV9zvJgqdHb"
      },
      "source": [
        "# Start of Clip Code\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "90exzNUlqnsL",
      "metadata": {
        "id": "90exzNUlqnsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3c5007-ab70-4525-8ea6-e977afc8088d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-htgkd8q7\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-htgkd8q7\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=e4ff8575d2255ad59cbfaaed8b3800068a4a43a2f13d141047ba7cb374c224e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rja57lg8/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5fe1c08a",
      "metadata": {
        "id": "5fe1c08a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QjRK2F8Zrh51",
      "metadata": {
        "id": "QjRK2F8Zrh51"
      },
      "source": [
        "## Training utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c5b0f7b",
      "metadata": {
        "id": "4c5b0f7b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aac2cf8",
      "metadata": {
        "id": "1aac2cf8"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cJ3Cq_hCsE5D",
      "metadata": {
        "id": "cJ3Cq_hCsE5D"
      },
      "outputs": [],
      "source": [
        "class model_classif(torch.nn.Module):\n",
        "    def __init__(self,model,n_out,feature_extraction=True):\n",
        "        super(model_classif, self).__init__()\n",
        "        self.model_pretrained = model # The pretrained model\n",
        "\n",
        "        set_parameter_requires_grad(self.model_pretrained,feature_extraction)\n",
        "        self.fc = nn.Linear(self.model_pretrained.output_dim,n_out)  # The output layer we add\n",
        "        self.gle = nn.GELU()  # The activation function\n",
        "\n",
        "    def forward(self,x):\n",
        "        res0=self.model_pretrained(x)\n",
        "        res1=self.fc(res0)\n",
        "        res2=self.gle(res1)\n",
        "\n",
        "        return res2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tIwDHHL0r40E",
      "metadata": {
        "id": "tIwDHHL0r40E"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3c711c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3c711c",
        "outputId": "93b471ce-e047-4110-f05d-9213d0bc5e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 120MiB/s]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed61bf26",
      "metadata": {
        "id": "ed61bf26"
      },
      "outputs": [],
      "source": [
        "# Number of classes in the dataset\n",
        "num_classes = 15\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "\n",
        "data_dir = './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3001c88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3001c88",
        "outputId": "0c7dc461-ae0c-489c-9fb7-8ee007ba8836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), preprocess) for x in ['train', 'test']}\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
        "\n",
        "# Detect if the GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m-cvkE6GrMaZ",
      "metadata": {
        "id": "m-cvkE6GrMaZ"
      },
      "source": [
        "## First Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65819416",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65819416",
        "outputId": "8db19f04-7a92-4577-b0ce-6bab6bff24a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "feature_extract = True\n",
        "\n",
        "model_feat_extr_1 = model_classif(model.visual,n_out=num_classes,feature_extraction=feature_extract)\n",
        "input_size = model.visual.input_resolution\n",
        "\n",
        "\n",
        "\n",
        "model_feat_extr_1 = model_feat_extr_1.to(device).float()\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_feat_extr_1.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_feat_extr_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_feat_extr_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_feat_extr_1 = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68ba017",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68ba017",
        "outputId": "ec72bab2-6684-471c-e399-57a4147b7aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.8640 Acc: 0.9796\n",
            "test Loss: 0.6251 Acc: 0.9764\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.4925 Acc: 0.9895\n",
            "test Loss: 0.4038 Acc: 0.9869\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.3346 Acc: 0.9921\n",
            "test Loss: 0.2976 Acc: 0.9895\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.2528 Acc: 0.9934\n",
            "test Loss: 0.2362 Acc: 0.9921\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.2041 Acc: 0.9921\n",
            "test Loss: 0.1958 Acc: 0.9895\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.1718 Acc: 0.9934\n",
            "test Loss: 0.1705 Acc: 0.9921\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.1483 Acc: 0.9941\n",
            "test Loss: 0.1508 Acc: 0.9921\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.1316 Acc: 0.9947\n",
            "test Loss: 0.1364 Acc: 0.9921\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.1180 Acc: 0.9941\n",
            "test Loss: 0.1243 Acc: 0.9921\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.1073 Acc: 0.9941\n",
            "test Loss: 0.1150 Acc: 0.9921\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.0986 Acc: 0.9954\n",
            "test Loss: 0.1075 Acc: 0.9921\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.0912 Acc: 0.9954\n",
            "test Loss: 0.1006 Acc: 0.9921\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.0852 Acc: 0.9954\n",
            "test Loss: 0.0955 Acc: 0.9921\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.0799 Acc: 0.9947\n",
            "test Loss: 0.0907 Acc: 0.9921\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.0750 Acc: 0.9961\n",
            "test Loss: 0.0864 Acc: 0.9921\n",
            "\n",
            "Training complete in 4m 46s\n",
            "Best val Acc: 0.992147\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_feat_extr_1, hist_feat_extr_1 = train_model(model_feat_extr_1, dataloaders_dict, criterion, optimizer_feat_extr_1, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd88f9a",
      "metadata": {
        "id": "7cd88f9a"
      },
      "source": [
        "## Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a788e89",
      "metadata": {
        "id": "5a788e89"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167039f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "167039f9",
        "outputId": "dcd6e1bf-456f-467c-853b-335913e854ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t model_pretrained.class_embedding\n",
            "\t model_pretrained.positional_embedding\n",
            "\t model_pretrained.proj\n",
            "\t model_pretrained.conv1.weight\n",
            "\t model_pretrained.ln_pre.weight\n",
            "\t model_pretrained.ln_pre.bias\n",
            "\t model_pretrained.transformer.resblocks.0.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.0.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.0.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.0.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.0.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.0.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.0.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.0.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.0.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.0.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.0.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.0.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.1.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.1.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.1.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.1.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.1.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.1.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.1.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.1.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.1.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.1.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.1.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.1.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.2.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.2.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.2.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.2.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.2.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.2.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.2.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.2.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.2.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.2.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.2.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.2.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.3.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.3.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.3.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.3.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.3.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.3.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.3.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.3.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.3.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.3.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.3.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.3.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.4.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.4.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.4.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.4.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.4.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.4.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.4.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.4.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.4.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.4.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.4.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.4.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.5.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.5.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.5.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.5.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.5.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.5.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.5.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.5.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.5.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.5.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.5.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.5.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.6.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.6.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.6.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.6.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.6.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.6.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.6.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.6.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.6.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.6.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.6.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.6.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.7.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.7.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.7.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.7.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.7.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.7.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.7.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.7.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.7.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.7.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.7.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.7.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.8.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.8.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.8.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.8.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.8.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.8.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.8.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.8.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.8.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.8.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.8.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.8.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.9.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.9.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.9.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.9.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.9.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.9.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.9.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.9.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.9.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.9.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.9.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.9.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.10.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.10.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.10.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.10.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.10.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.10.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.10.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.10.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.10.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.10.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.10.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.10.ln_2.bias\n",
            "\t model_pretrained.transformer.resblocks.11.attn.in_proj_weight\n",
            "\t model_pretrained.transformer.resblocks.11.attn.in_proj_bias\n",
            "\t model_pretrained.transformer.resblocks.11.attn.out_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.11.attn.out_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.11.ln_1.weight\n",
            "\t model_pretrained.transformer.resblocks.11.ln_1.bias\n",
            "\t model_pretrained.transformer.resblocks.11.mlp.c_fc.weight\n",
            "\t model_pretrained.transformer.resblocks.11.mlp.c_fc.bias\n",
            "\t model_pretrained.transformer.resblocks.11.mlp.c_proj.weight\n",
            "\t model_pretrained.transformer.resblocks.11.mlp.c_proj.bias\n",
            "\t model_pretrained.transformer.resblocks.11.ln_2.weight\n",
            "\t model_pretrained.transformer.resblocks.11.ln_2.bias\n",
            "\t model_pretrained.ln_post.weight\n",
            "\t model_pretrained.ln_post.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "feature_extract = False\n",
        "\n",
        "model_ft_1 = model_classif(model.visual,n_out=num_classes,feature_extraction=feature_extract)\n",
        "input_size = model.visual.input_resolution\n",
        "\n",
        "\n",
        "\n",
        "model_ft_1 = model_ft_1.to(device).float()\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft_1.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft_1 = optim.SGD(params_to_update, lr=1e-5, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af38ee0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af38ee0d",
        "outputId": "abf35307-d9b3-4edb-ed02-248fb149a1b0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 2.5660 Acc: 0.1803\n",
            "test Loss: 2.3930 Acc: 0.2277\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 2.2253 Acc: 0.3092\n",
            "test Loss: 2.0351 Acc: 0.4791\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 1.8158 Acc: 0.6684\n",
            "test Loss: 1.6234 Acc: 0.7827\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 1.3962 Acc: 0.8724\n",
            "test Loss: 1.2287 Acc: 0.9136\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 1.0271 Acc: 0.9539\n",
            "test Loss: 0.9152 Acc: 0.9869\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.7499 Acc: 0.9934\n",
            "test Loss: 0.6956 Acc: 0.9895\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.5610 Acc: 0.9980\n",
            "test Loss: 0.5465 Acc: 0.9921\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.4321 Acc: 1.0000\n",
            "test Loss: 0.4440 Acc: 0.9921\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.3437 Acc: 1.0000\n",
            "test Loss: 0.3714 Acc: 0.9921\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.2811 Acc: 1.0000\n",
            "test Loss: 0.3161 Acc: 0.9921\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.2355 Acc: 1.0000\n",
            "test Loss: 0.2774 Acc: 0.9921\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.2017 Acc: 1.0000\n",
            "test Loss: 0.2465 Acc: 0.9921\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.1760 Acc: 1.0000\n",
            "test Loss: 0.2235 Acc: 0.9921\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.1556 Acc: 1.0000\n",
            "test Loss: 0.2019 Acc: 0.9921\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.1392 Acc: 1.0000\n",
            "test Loss: 0.1864 Acc: 0.9921\n",
            "\n",
            "Training complete in 10m 55s\n",
            "Best val Acc: 0.992147\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft_1, hist_ft_1 = train_model(model_ft_1, dataloaders_dict, criterion, optimizer_ft_1, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpfZaXdPrQkw",
      "metadata": {
        "id": "dpfZaXdPrQkw"
      },
      "source": [
        "## Plot Models Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QB5uYMgktBtX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB5uYMgktBtX",
        "outputId": "dd659efc-7ffd-4b9a-c025-f88fab44faff",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9764397905759163"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hist_feat_extr_1[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L5ic7DtooYvO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "L5ic7DtooYvO",
        "outputId": "d6107568-f147-4336-b434-f07e7a2cc5c6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig=plt.figure()\n",
        "\n",
        "plt.plot(range(15),hist_feat_extr_1,label='feature extracting')\n",
        "plt.plot(range(15),hist_ft_1,label='fine tuning SGD')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('val acc')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666532a4",
      "metadata": {
        "id": "666532a4"
      },
      "source": [
        "# One shot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "data_dir = './data/'\n",
        "\n",
        "image_datasets_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"test_os\"), preprocess)\n",
        "dataloaders_one_shot  = torch.utils.data.DataLoader(image_datasets_one_shot, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "image_datasets_train_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"train_os\"), preprocess)\n",
        "dataloaders_train_one_shot  = torch.utils.data.DataLoader(image_datasets_one_shot, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "# Detect if the GPU is available\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUJGq9I3lkVQ",
        "outputId": "e9993eb0-391f-4bdc-fd43-4918880e6a98"
      },
      "id": "TUJGq9I3lkVQ",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets_train_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"train_os\"), preprocess)\n",
        "dataloaders_train_one_shot  = torch.utils.data.DataLoader(image_datasets_train_one_shot, batch_size=1, shuffle=False, num_workers=4)\n",
        "# Calculate features\n",
        "success = 0\n",
        "nb_tot = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_features = torch.cat([model.encode_image(inputs.to(device)) for inputs, labels in dataloaders_train_one_shot])\n",
        "  train_features /= train_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  for inputs, labels in dataloaders_one_shot:\n",
        "    inputs = inputs.to(device)\n",
        "    image_features = model.encode_image(inputs)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * image_features @ train_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(1)\n",
        "    if indices.item() == labels.item():\n",
        "      success += 1\n",
        "    nb_tot += 1\n",
        "\n",
        "accuracy = success/nb_tot\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fHi_f2m1tV",
        "outputId": "e9ab979e-95f1-4f58-cd15-5048caaf78ae"
      },
      "id": "e8fHi_f2m1tV",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9798792756539235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "dcda5fee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcda5fee",
        "outputId": "27f19e1c-d617-4573-b000-429319201a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1851106639839034\n"
          ]
        }
      ],
      "source": [
        "# Calculate features\n",
        "success = 0\n",
        "nb_tot = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in oneshot_cats]).to(device)\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  for inputs, labels in dataloaders_one_shot:\n",
        "    inputs = inputs.to(device)\n",
        "    image_features = model.encode_image(inputs)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(1)\n",
        "    if indices.item() == labels.item():\n",
        "      success += 1\n",
        "    nb_tot += 1\n",
        "\n",
        "accuracy = success/nb_tot\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxJigVIDjgvQ",
        "outputId": "68495fec-6af3-42a0-cf35-b3e86ce677dc"
      },
      "id": "xxJigVIDjgvQ",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oneshot_cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEc-f6qplAQg",
        "outputId": "9dcec2da-8e31-4200-b95a-d15c9971d439"
      },
      "id": "FEc-f6qplAQg",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DuckHead', 'WolfHead', 'PigHead', 'LionHead', 'EagleHead']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V20d7XhFlM06"
      },
      "id": "V20d7XhFlM06",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7cd88f9a",
        "dpfZaXdPrQkw"
      ],
      "name": "Clip_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}